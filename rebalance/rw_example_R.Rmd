---
title: "A Simple Example Where re-Weighting Data is Not Monotone"
author: "John Mount, Nina Zumel; https://www.win-vector.com"
date: "`r date()`"
output: github_document
---

## Introduction

Here is an example of how re-weighting data as function of the training outcome to balance the positive and negative examples can change results in a non-monotone manner, even for a simple logistic regression.  This means in this case re-weighing is not equivalent to sweeping the chosen threshold that converts a score into a classification rule. However, [for single variable plus intercept logistic regression models no such non-monotone re-ordering is possible](https://github.com/WinVector/Examples/blob/main/rebalance/rw_invariant.md).

[Nina Zumel had some interesting comments on the ideas](https://ninazumel.com/2015/02/27/balancing-classes-before-training-classifiers-addressing-a-folk-theorem/) which lead us to conclude: if re-balancing does anything better than moving your threshold, this is in fact evidence of a missed interaction.

It is our thesis that their is little benefit to re-balancing data and if there appears to be such a benefit it means you failded to use numeric scores (converted to a classification rule too early) or missed an interaction in your data (which can be fixed by a bit more feature engineering, the non-montone change suggests some interactions that can be introduced).

## Example 

Let's work our example in [`R`](https://www.r-project.org). Some of our terminology is defined in our [companion note](https://github.com/WinVector/Examples/blob/main/rebalance/rw_invariant.md).

```{r}
# first attach packages
library(wrapr)
library(WVPlots)
```

```{r}
# build our example data
# modeling y as a function of x1 and x2 (plus intercept)

d <- wrapr::build_frame(
  "x1"  , "x2", "y", "w2" |
    0   , 0   , 0  , 2    |
    0   , 0   , 0  , 2    |
    0   , 1   , 1  , 5    |
    1   , 0   , 0  , 2    |
    1   , 0   , 0  , 2    |
    1   , 0   , 1  , 5    |
    1   , 1   , 0  , 2    )

knitr::kable(d)
```

### First Model

Fit a logistic regression model

```{r}
model1 <- glm(
  y ~ x1 + x2,
  data = d,
  family = binomial())
```

Take a look at the model summary.

```{r}
summary(model1)
```

Add the model predictions to the data frame.

```{r}
d$pred1 <- predict(model1, newdata = d, type = 'response')
```

```{r}
knitr::kable(d)
```

However, notice the was not a "balanced" problem or a problem with prevalence equal to 0.5.

```{r}
sum(d$y) / nrow(d)
```

### Re-balanced Model

Let's see if fitting a balanced copy of the data set (created by up-sampling the positive examples) gives us a structurally different answer.

```{r}
# confirm weighted prevalence is exactly 0.5
sum(d$y * d$w2) / sum(d$w2)
```

```{r}
model2 <- glm(
  y ~ x1 + x2,
  data = d,
  weights = w2,
  family = binomial())
```

```{r}
summary(model2)
```

```{r}
# land the new predictions in our data frame
d$pred2 <- predict(model2, newdata = d, type = 'response')
```

```{r}
knitr::kable(d)
```

### The Difference

Notice rows 1 and 2 are predicted to have larger probability (prediction ~ 0.23) in model1 than rows 4 and 5 (prediction ~ 0.18).  This relation is reversed in model2. So the models have essentially different order, and therefore are not monotone transforms of each other.

This can also be seen in the ROC plots.

```{r}
ROCPlotPair(
  d,
  xvar1 = 'pred1',
  xvar2 = 'pred2',
  truthVar = 'y',
  truthTarget = 1,
  title = 'ROC')
```

We see the regular model is better in the high-specificity / low-sensitivity region and the balanced model is better at the low-specificity / high-sensitivity regions. However, this can be misleading as the ROC plot deliberately removes prevalence driven effects.

We can also look at precision and recall trade-offs.

```{r}
PRPlot(
  d,
  xvar = 'pred1',
  truthVar = 'y',
  truthTarget = 1,
  title = 'pred1 precision and recall')
```

```{r}
PRPlot(
  d,
  xvar = 'pred2',
  truthVar = 'y',
  truthTarget = 1,
  title = 'pred2 precision and recall')
```

## A Critique

An important property of logistic regression is [the balance properties](https://win-vector.com/2011/09/14/the-simpler-derivation-of-logistic-regression/): for any variable `v` we have `sum(d[[v]] * d$y) == sum(d[[v]] * d$prediction)`. Fitting with the balance priors (essentially the wrong priors loses this property).

```{r}
sum(d$x1 * d$y)
```

```{r}
sum(d$x1 * d$pred1)  # matches sum(d$x1 * d$y)
```

```{r}
sum(d$x1 * d$pred2)  # does not match
```

And this is not fixed by trying to adjust back to the true (unbalanced) priors.

```{r}
sum(d$x1 * d$pred2) * (  sum(d$y) / sum(d$pred2) )  # still does not match
```

## Moving Forward

It is not obvious that re-scaling is always going to be a bad transform. But our point is: it is not obvious re-scaling is always going to be a good transform. As we have [written before](https://win-vector.com/2020/08/07/dont-use-classification-rules-for-classification-problems/). It is our opinion that data re-sampling is often used to work around the avoidable mistake of using a classification rule where a detailed numeric score would in fact to better.


As we mentioned in our [invariant note](https://github.com/WinVector/Examples/blob/main/rebalance/rw_invariant.md) a saturated version of this data set will not have the non-monotone property. With enough training data the satuarating is mere feature engineering.  The non-monotone transform is a symptom of a missing interaction forcing the modeling to make different compromises at different data prevalences. With a richer feature set the model can make different decisions for subsets of rows, yielding a better model with fewer compromises.

### Adding Interactions

The non-monotone set can actually suggest interactions to add.


```{r}
# find the order changes.
combs <- combn(seq_len(nrow(d)), 2)
reversals <- ( d$pred1[combs[1, ]] > d$pred1[combs[2, ]] ) & 
  ( d$pred2[combs[1, ]] < d$pred2[combs[2, ]] )
pairs <- combs[, reversals, drop = FALSE]
pairs
```

`pairs` is the edge-set of a graph where the top ends of edges are greater than the bottom ends in the `pred1` order and also less than the bottom ends in the `pred2` order. The components of this graph are bipartite, and we want new variables eliminate edges from these graphs.

In our case the supports of the bipartite components are `{({1,2}, {4, 5, 6}), ({3}, {7})}`. Our idea is to introduce variables that identify these support sets, as this would give the model the degrees of freedom needed to re-score these sets independently and remove the compromises forcing the non-monotone change.

```{r}
d$s_456 <- d$x1 - d$x1 * d$x2
d$s_12 <- (1 - d$x1) * (1 - d$x2)
d$s_3 <- (1 - d$x1) * d$x2
d$s_7 <- d$x1 * d$x2
knitr::kable(d)
```

The idea is: we are working in the set-algebra of the indicator variables.  Complement is subtraction from one, intersection is represented by multiplication, union compliment of intersection of complements. The supports must be in this algebra as rows that are indistinguishible by combinations of our variables must enter and leave the supports together.

Once we have an augmented set of variables we can re-solve for models under both data weightings.

```{r}
model1s <- glm(
  y ~ x1 + x2 + s_456 + s_12 + s_3 + s_7,
  data = d,
  family = binomial())
```

```{r}
predict(model1s, newdata = d, type = 'response')
```

```{r}
model2s <- glm(
  y ~ x1 + x2 + s_456 + s_12 + s_3 + s_7,
  data = d,
  weights = w2,
  family = binomial())
```


```{r}
predict(model2s, newdata = d, type = 'response')
```

Notice the two predictions, while different, now have the same order-statistics. So instead of worrying which of the original two models was better, we instead say the original order difference were in fact evidence of missing interaction variables. With the additional interaction variables we have a model structure, that with enough training data, should dominate both original models.

The point is: with individual variables that contain finer detail about the data fewer trade-offs are required, not leaving in the possibility of a non-monotone.  Likely higher complexity models such as polynomial regression, kernelized methods, tree based methods, ensemble methods, and neural nets introduce enough interactions to not fundamentally need the re-balance (though any one particular implementation may fall short).

This can be achieved all at once by introducing the obvious categorical variable that the partition implied by the combinations of the original variables variables.

```{r}
d$cat <- paste(d$x1, d$x2)
```

```{r}
model1c <- glm(
  y ~ cat,
  data = d,
  family = binomial())
```

```{r}
d$pred1c <- predict(model1c, newdata = d, type = 'response')
d$pred1c
```

```{r}
model2c <- glm(
  y ~ cat,
  data = d,
  weights = w2,
  family = binomial())
```


```{r}
d$pred2c <- predict(model2c, newdata = d, type = 'response')
d$pred2c
```

## Shifting

Matloff *Statistical Regression and Classification*, CRC Press, 2017, section  5.8.2.2 "The Issue of 'Unbalanced (and Balanced) Data, Remedies" suggests shifting prediction scores instead of moving thresholds to deal changes in priors. This is good advice and emphasizes staying with scores, instead of binding in thresholds to form classification rules too early.

The monotone models are very well suited for this transform, as for them any such transform is order equivilent to re-training with different priors.

## Conclusion

We have shown some ways to refine the logistic regression model so it is prediction order-invariant to data prevalence. This sort of model should be prefered.

