---
title: "A Simple Example Where re-Weighting Data is Not Monotone"
author: "John Mount, Nina Zumel; https://www.win-vector.com"
date: "`r date()`"
output: github_document
---

Here is an example of how re-weighting data as function of the training outcome to balance the positive and negative examples can change results in a non-monotone manner, even for a simple logistic regression.  This means in this case re-weighing is not equivalent to sweeping the chosen threshold that converts a score into a classification rule. However, [for single variable plus intercept logistic regression models no such non-monotone re-ordering is possible](https://github.com/WinVector/Examples/blob/main/rebalance/rw_invariant.md). 

[Nina Zumel had some interesting comments on the ideas](https://ninazumel.com/2015/02/27/balancing-classes-before-training-classifiers-addressing-a-folk-theorem/) which lead us to conclude: if re-balancing does anything better than moving your threshold, this is in fact evidence of a missed interaction.

It is our thesis that their is little benefit to re-balancing data and if there appears to be such a benefit it means you failded to use numeric scores (converted to a classification rule too early) or missed an interaction in your data (which can be fixed by a bit more feature engineering, the non-montone change suggests some interactions that can be introduced).

Let's work our example in [`R`](https://www.r-project.org).

```{r}
# first attach packages
library(wrapr)
library(WVPlots)
```

```{r}
# build our example data
# modeling y as a function of x1 and x2 (plus intercept)

d <- wrapr::build_frame(
  "x1"  , "x2", "y", "w2" |
    0   , 0   , 0  , 2    |
    0   , 0   , 0  , 2    |
    0   , 1   , 1  , 5    |
    1   , 0   , 0  , 2    |
    1   , 0   , 0  , 2    |
    1   , 0   , 1  , 5    |
    1   , 1   , 0  , 2    )

knitr::kable(d)
```

Fit a logistic regression model

```{r}
model1 <- glm(
  y ~ x1 + x2,
  data = d,
  family = binomial())
```

Take a look at the model summary.

```{r}
summary(model1)
```

Add the model predictions to the data frame.

```{r}
d$pred1 <- predict(model1, newdata = d, type = 'response')
```

```{r}
knitr::kable(d)
```

However, notice the was not a "balanced" problem or a problem with prevalence equal to 0.5.

```{r}
sum(d$y) / nrow(d)
```

Let's see if fitting a balanced copy of the data set (created by up-sampling the positive examples) gives us a structurally different answer.

```{r}
# confirm weighted prevalence is exactly 0.5
sum(d$y * d$w2) / sum(d$w2)
```

```{r}
model2 <- glm(
  y ~ x1 + x2,
  data = d,
  weights = w2,
  family = binomial())
```

```{r}
summary(model2)
```

```{r}
# land the new predictions in our data frame
d$pred2 <- predict(model2, newdata = d, type = 'response')
```

```{r}
knitr::kable(d)
```

Notice rows 1 and 2 are predicted to have larger probability (prediction ~ 0.23) in model1 than rows 4 and 5 (prediction ~ 0.18).  This relation is reversed in model2. So the models have essentially different order, and therefore are not monotone transforms of each other.

This can also be seen in the ROC plots.

```{r}
ROCPlotPair(
  d,
  xvar1 = 'pred1',
  xvar2 = 'pred2',
  truthVar = 'y',
  truthTarget = 1,
  title = 'ROC')
```

We see the regular model is better in the high-specificity / low-sensitivity region and the balanced model is better at the low-specificity / high-sensitivity regions. However, this can be misleading as the ROC plot deliberately removes prevalence driven effects.

We can also look at precision and recall trade-offs.

```{r}
PRPlot(
  d,
  xvar = 'pred1',
  truthVar = 'y',
  truthTarget = 1,
  title = 'pred1 precision and recall')
```

```{r}
PRPlot(
  d,
  xvar = 'pred2',
  truthVar = 'y',
  truthTarget = 1,
  title = 'pred2 precision and recall')
```


It is not obvious that re-scaling is always going to be a bad transform. But our point is: it is not obvious re-scaling is always going to be a good transform. As we have [written before](https://win-vector.com/2020/08/07/dont-use-classification-rules-for-classification-problems/). It is our opinion that data re-sampling is often used to work around the avoidable mistake of using a classification rule where a detailed numeric score would in fact to better.


As we mentioned in our [invariant note](https://github.com/WinVector/Examples/blob/main/rebalance/rw_invariant.md) a saturated version of this data set will not have the non-monotone property. With enough training data the satuarating is mere feature engineering.

For example we get rid of the non-monotone change (and claimed advantage) by adding a few interaction variables (it is not necessary to fully saturate the system).

```{r}
d$x3 <- d$x1 * d$x2
d$x4 <- (1 - d$x1) * d$x2
d$x5 <- d$x1 * (1 - d$x2)
d$x6 <- (1 - d$x1) * (1 - d$x2)

knitr::kable(d)
```

```{r}
model1s <- glm(
  y ~ x1 + x2 + x3 + x4 + x5 + x6,
  data = d,
  family = binomial())
```

```{r}
predict(model1s, newdata = d, type = 'response')
```

```{r}
model2s <- glm(
  y ~ x1 + x2 + x3 + x4 + x5 + x6,
  data = d,
  weights = w2,
  family = binomial())
```


```{r}
predict(model2s, newdata = d, type = 'response')
```

Notice the two predictions have the same order-statistics.

The point is: with individual variables that contain finer detail about the data fewer trade-offs are required, not leaving in the possibility of a non-monotone.  Likely higher complexity models such as polynomial regression, kernelized methods, tree based methods, ensemble methods, and neural nets introduce enough interactions to not fundementally need the re-balance (though any one particular implementation may fall short).

This can be achieved quicker by introducing the obvious categorical variable that the partition implied by the satured variables.

```{r}
d$cat <- paste(d$x1, d$x2)
```

```{r}
model1c <- glm(
  y ~ cat,
  data = d,
  family = binomial())
```

```{r}
predict(model1c, newdata = d, type = 'response')
```

```{r}
model2c <- glm(
  y ~ cat,
  data = d,
  weights = w2,
  family = binomial())
```


```{r}
predict(model2c, newdata = d, type = 'response')
```
