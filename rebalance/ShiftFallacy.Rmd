---
title: "The Shift and Balance Fallacies"
output: github_document
---

Two related fallacies I see in machine learning practice are the shift and balance fallacies (for an earlier simple fallacy, please see [here](https://win-vector.com/2020/09/07/the-intercept-fallacy/)). They involve thinking logistic regression has a bit simpler structure that it actually does, and also thinking logistic regression is a bit less powerful than it actually is.

The fallacies are somewhat opposite: the first fallacy is shifting or re-weighting data doesn't change much, and the second is that re-balancing is a necessary pre-processing step. As the two ideas seem to contradict each other it would be odd if they were both true. In fact they are closer to both being false.

## The shift fallacy

The shift fallacy is as follows. We fit two models `m` and `m_shift` with data-weights `one` (the all ones vector) and `a * one + b * y` (`y` being the dependent variable). We are re-sampling according to outcome, a technique popular with some for un-balanced classification problems. Then it is (falsely) believed the two models differ only in the intercept term.

This is easy to disprove in [R](https://www.r-project.org).

```{r}
library(wrapr)

# build our example data
# modeling y as a function of x1 and x2 (plus intercept)

d <- wrapr::build_frame(
  "x1"  , "x2", "y" |
    0   , 0   , 0   |
    0   , 0   , 0   |
    0   , 1   , 1   |
    1   , 0   , 0   |
    1   , 0   , 0   |
    1   , 0   , 1   |
    1   , 1   , 0   )

knitr::kable(d)
```

First we fit the model with each data-row having the same weight.

```{r}
m <- glm(
  y ~ x1 + x2,
  data = d,
  family = binomial())

m$coefficients
```

Now we build a balanced weighting. We are up-sampling both classes so we don't have any fractional weights.

```{r}
w <- ifelse(d$y == 1, sum(1 - d$y), sum(d$y))
w
```

```{r}
# confirm prevalence is 0.5 under this weighting
sum(w * d$y) / sum(w)
```

Now we fit the model for the balanced data situation.

```{r}
m_shift <- glm(
  y ~ x1 + x2,
  data = d,
  family = binomial(),
  weights = w)

m_shift$coefficients
```

Notice that all coefficient changed, not just the intercept term. And we have thus demonstrated the shift fallacy.

## The balance fallacy

An additional point is: the simple model without re-weighting is the better model on this training data. There appears to be an industry belief that to work with unbalanced classes one must re-balance the data. In fact moving to "balanced data" doesn't magically improve the model quality, what it *does* is helps hide *some* of the bad consequences of using classification rules instead of probability models (please see [here](https://win-vector.com/2020/08/07/dont-use-classification-rules-for-classification-problems/) for some discussion).

For instance our original model has the following statistical deviance (lower is better):

```{r}
deviance <- function(prediction, truth) {
  -2 * sum(truth * log(prediction) + (1 - truth) * log(1 - prediction))
}

deviance(
  prediction = predict(m, newdata = d, type = 'response'),
  truth = d$y)
```

And our balanced model has a worse deviance.

```{r}
deviance(
  prediction = predict(m_shift, newdata = d, type = 'response'),
  truth = d$y)
```

Part of that is the balanced model is scaled wrong. It's average prediction is, by design, inflated.

```{r}
mean(predict(m_shift, newdata = d, type = 'response'))
```

Whereas, the original model average to the same as the average of the truth values ([a property of logistic regression](https://win-vector.com/2011/09/14/the-simpler-derivation-of-logistic-regression/)).

```{r}
mean(predict(m, newdata = d, type = 'response'))
```

```{r}
mean(d$y)
```

So let's adjust the balanced predictions back to the correct expected value.

```{r}
d$balanced_pred <- predict(m_shift, newdata = d, type = 'link')
m_scale <- glm(
  y ~ balanced_pred,
  data = d,
  family = binomial())

corrected_balanced_pred <- predict(m_scale, newdata = d, type = 'response')
mean(corrected_balanced_pred)
```

We now have a prediction with the correct expected value. However, notice this deviance is *still* larger than the simple un-weighted original model.

```{r}
deviance(
  prediction = corrected_balanced_pred,
  truth = d$y)
```

Our opinion is: re-weighting or re-sampling data for a logistic regression is pointless. The fitting procedure deals with un-balance quite well, and doesn't need any attempt at help. We think this sort of re-weighting and re-sampling introduces complexity, the possibility of data-leaks with up-sampling, and a loss of statistical efficiency with down-sampling. Likely the re-sampling fallacy is driven by a need to move model scores to near `0.5` when using `0.5` as a default classification rule threshold (which we argue against in ["Donâ€™t Use Classification Rules for Classification Problems"](https://win-vector.com/2020/08/07/dont-use-classification-rules-for-classification-problems/)).

## Conclusion

Some tools, such as logistic regression, work best on training data that accurately represents the problem and do not require artificially balanced training data. Re-balancing training data is a bit more involved that one might thing, as we see more than just the intercept term changes when we re-balance data.

Take logistic regression as the entry level probability model for classification problems. If it doesn't need data re-balancing then other tools may also not need it (though if they are internally using classification rule metrics, some hyper-parameters or internal procedures may need to be adjusted).



